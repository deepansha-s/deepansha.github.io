---
layout: page
permalink: /research/
title: research
description: research experiences.
nav: true
nav_order: 2
---

I started doing CS research in high school after cold-emailing Rutgers faculty, which led to an early graph theory project. Since then, nearly all of my work has focused on machine learning. Along the way I’ve built a strong bias toward open-ended problem solving and grit.  
**Recognition:** 2023–2024 UC San Diego Physical Sciences Dean’s Undergraduate Award for Excellence (nominated by my thesis advisor).

---

## Esoteric Language Models

**Kuleshov Lab, Cornell University (Cornell Tech)** — contributed across several projects:

- Studied diffusion and “next-gen” language models (e.g., MDLM, MuLAN); surveyed samplers and schedules.
- Implemented interpolation between reconstruction and diffusion components at different timesteps.
- Modified attention-masking mechanisms for a diffusion-transformer denoiser in a targeted use case.
- Ran Conditional Mauve evaluations; integrated external samplers (e.g., ReMDM) into our codebase.
- Engineered clean Torch code, ran experiments across configs, analyzed metrics and samples.

## Google Computer Science Research Mentorship Program (CSRMP)

**Google (Remote)**

- Mentored by a senior scientist (DeepMind) on core ML topics; worked through selected chapters/resources.
- Broadened perspective on current research directions and solidifed research concepts.

## Flat Minima Can Fail to Transfer to Downstream Tasks

**Voluntarily advised by a DeepMind senior research scientist & Vector Institute co-director**

- Reviewed related work (e.g., Tengyu Ma’s papers) to build foundations.
- Tested an edge case from prior work; observed generalization/optimization behavior under perturbations.
- Implemented noise/perturbation pipelines; ran MNIST/CIFAR experiments with ResNets and other models.

## ConverSense: Automated Assessment of Patient–Provider Interactions via Social Signals

**Weibel Lab, UC San Diego**

- Surveyed feature-extraction approaches for social-signal detection; tried NVIDIA diarization pipelines.
- Used **librosa** to compute audio features; inspected importance patterns (e.g., with seaborn).
- Trained **scikit-learn** classifiers; addressed imbalance with **SMOTE**.
- Post-submission, explored language-model techniques for healthcare applications.

## CSNext Program

**University of Washington, Reality Lab (Remote)**

- Deepened understanding of RNNs/LSTMs; worked with ECG data and classical ML classifiers.
- Presented final results to the group.

## Exploring Gradient-Free Optimization Algorithms

**Dr. Yuhua Zhu Lab (Math Dept Honors Thesis)**

- Studied federated learning and physics-inspired, gradient-free optimization.
- Implemented the algorithm (from pseudocode) in PyTorch; debugged and ran on several datasets.
- Built visualizations of consensus trajectories; parallelized heavy computations for the lab repo.
