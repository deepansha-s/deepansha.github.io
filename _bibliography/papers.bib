---
---
@misc{sahoo2025esotericlanguagemodels,
  bibtex_show={true},
  preview={eso-lm.svg},
  title={Esoteric Language Models}, 
  author={Subham Sekhar Sahoo and Zhihan Yang and Yash Akhauri and Johnna Liu and Deepansha Singh and Zhoujun Cheng and Zhengzhong Liu and Eric Xing and John Thickstun and Arash Vahdat},
  year={2025},
  eprint={2506.01928},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2506.01928}, 
  note = {<em><b> I contributed as joint second author. </b></em>},
  keywords = {diffusion language models, language models, generative models},
  doi = {10.48550/arXiv.2506.01928},
  abstract = {Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features—most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the first to introduce KV caching for MDMs while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to 65× faster inference than standard MDMs and 4× faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: https://s-sahoo.com/Eso-LMs}
  }

@inproceedings{10.1145/3613904.3641998,
  bibtex_show={true},
  author = {Bedmutha, Manas Satish and Tsedenbal, Anuujin and Tobar, Kelly and Borsotto, Sarah and Sladek, Kimberly R and Singh, Deepansha and Casanova-Perez, Reggie and Bascom, Emily and Wood, Brian and Sabin, Janice and Pratt, Wanda and Hartzler, Andrea and Weibel, Nadir},
  title = {ConverSense: An Automated Approach to Assess Patient-Provider Interactions using Social Signals},
  year = {2024},
  isbn = {9798400703300},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3613904.3641998},
  doi = {10.1145/3613904.3641998},
  abstract = {Patient-provider communication influences patient health outcomes, and analyzing such communication could help providers identify opportunities for improvement, leading to better care. Interpersonal communication can be assessed through “social-signals” expressed in non-verbal, vocal behaviors like interruptions, turn-taking, and pitch. To automate this assessment, we introduce a machine-learning pipeline that ingests audio-streams of conversations and tracks the magnitude of four social-signals: dominance, interactivity, engagement, and warmth. This pipeline is embedded into ConverSense, a web-application for providers to visualize their communication patterns, both within and across visits. Our user study with 5 clinicians and 10 patient visits demonstrates ConverSense’s potential to provide feedback on communication challenges, as well as the need for this feedback to be contextualized within the specific underlying visit and patient interaction. Through this novel approach that uses data-driven self-reflection, ConverSense can help providers improve their communication with patients to deliver improved quality of care.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {448},
  numpages = {22},
  keywords = {healthcare, interactions, patient-provider communication, social signals},
  location = {Honolulu, HI, USA},
  series = {CHI '24}
}


@misc{singh2025workshop,
  bibtex_show={true},
  title        = {Flat minima can fail to transfer to downstream tasks},
  author       = {Deepansha Singh, Ekansh Sharma, Daniel M. Roy, Gintare Karolina Dziugaite},
  year         = {2023},
  howpublished = {Presented at the XYZ Workshop on FooBar, ICML 2025 (non-archival)},
  abstract = {Large neural networks trained on one task are often finetuned and reused on different but related downstream tasks. The prospect of general principals that might lead to improved transferability is very enticing, as pretraining is exceptionally resource intensive. In recent work, Liu et al. (2022) propose to use flatness as a metric to judge the transferability of pretrained neural networks, based on the observation that, on a suite of benchmarks, flatter minima led to better transfer. Is this a general principal? In this extended abstract, we show that flatness is not a reliable indicator of transferability, despite flatness having been linked to generalization via PAC-Bayes and empirical analysis. We demonstrate that the question of whether flatness helps or hurts depends on the relationship between the source and target tasks.},
  keywords = {transfer learning, flat minima, loss landscape, deep learning theory, computer vision},
}
